{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testbed for netcdf processing efficiency\n",
    "- Use Dask\n",
    "- Chunk\n",
    "- Only process data to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "# Add common resources folder to path\n",
    "sys.path.append(\"/mnt/mcc-ns9600k/jonahks/git_repos/netcdf_analysis/Common/\")\n",
    "sys.path.append(\"/mnt/mcc-ns9600k/jonahks/git_repos/netcdf_analysis/\")\n",
    "\n",
    "from imports import (\n",
    "    pd, np, xr, mpl, plt, sns, os, \n",
    "    datetime, sys, crt, gridspec,\n",
    "    polyfit, ccrs, LinearRegression, metrics\n",
    "    )\n",
    "\n",
    "from functions import (\n",
    "    masked_average, interpretNS, plot_slf_isotherms, \n",
    "    add_weights, process_caliop, process_for_slf,\n",
    "    noresm_slf_to_df, regress_1d\n",
    "    )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running through MC2 Jupyter Hub\n",
      "Can access all directory paths: True\n"
     ]
    }
   ],
   "source": [
    "host = os.uname()[1]\n",
    "if 'jupyter' in host.split('-'): # Check if running on NIRD through the Jupyter Hub\n",
    "    print('Running through MC2 Jupyter Hub')\n",
    "    model_dir = '/mnt/mcc-ns9600k/jonahks/'\n",
    "    os.chdir(model_dir)\n",
    "\n",
    "else:  # Assume that we're running on a local machine and mounting NIRD\n",
    "    print('Running on %s, attempting to mount ns9600k/jonahks/ from NIRD' % str(host))\n",
    "    os.system('fusermount -zu ~/drivemount/')  # unmount first\n",
    "    os.system('sshfs jonahks@login.nird.sigma2.no:\"p/jonahks/\" ~/drivemount/')    # Calling mountnird from .bashrc doesn't work\n",
    "    os.chdir('/home/jonahks/drivemount/')\n",
    "    save_dir = '~/DATAOUT/'\n",
    "    save_to = os.path.expanduser(save_dir)\n",
    "\n",
    "output_dir = 'figures/'\n",
    "case_dir = 'mnth15runs/'   # inconsistent label compared to jupy_test\n",
    "mods_dir = 'inp_validation/'\n",
    "\n",
    "# Check that each important directory can be accessed:    \n",
    "access_paths = os.path.exists(mods_dir) and os.path.exists(output_dir) and os.path.exists(model_dir)\n",
    "print('Can access all directory paths:', access_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200112_002538_singleparam_nudge_wbf_1_inp_0',\n",
       " '20200116_130416_nudged_wbfmods_wbf_10_inp_1',\n",
       " '20191230_130025_singleparam_cttest15_wbf_1_inp_1',\n",
       " '20191217_145440_singleparam_jolt_wbf_1_inp_1000',\n",
       " '20200110_142006_singleparam_nudge_wbf_1_inp_1000',\n",
       " '20191128_171713_sampleparamset_wbf_10_inp_1',\n",
       " '20191209_180424_sampleparamset_wbf_1_inp_0.1',\n",
       " 'error_vs_iso.png',\n",
       " '.ipynb_checkpoints',\n",
       " 'unused',\n",
       " '20191127_162007_sampleparamset_wbf_0.01_inp_1',\n",
       " 'runs_as_vectors.png',\n",
       " '20191210_152149_sampleparamset_wbf_1_inp_0.1',\n",
       " '20200109_1541_wbf_1.0_inp_1.0',\n",
       " '20200128_142303_singleparam_frzrtvarsm15_wbf_1_inp_100',\n",
       " '20191128_171713_sampleparamset_wbf_1_inp_10',\n",
       " '20200204_120214_singleparam_wbfcheck_wbf_0.1_inp_1',\n",
       " '20191219_151155_singleparam_cttest_wbf_1_inp_1.cam.h0.0001-01',\n",
       " '20200204_113441_singleparam_inpcheck_wbf_1_inp_10',\n",
       " '20191210_152149_sampleparamset_wbf_1_inp_10',\n",
       " '20191217_134307_singleparam_jolt_wbf_1_inp_0',\n",
       " '20200116_130416_nudged_wbfmods_wbf_0.01_inp_1',\n",
       " 'paramspace.png',\n",
       " '20191122_161009_sampleparamset_wbf_1_inp_1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(case_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ['20200109_1541_wbf_1.0_inp_1.0', \n",
    "         '20200110_142006_singleparam_nudge_wbf_1_inp_1000',\n",
    "         '20200112_002538_singleparam_nudge_wbf_1_inp_0',\n",
    "         '20200116_130416_nudged_wbfmods_wbf_0.01_inp_1',\n",
    "         '20200204_113441_singleparam_inpcheck_wbf_1_inp_10',\n",
    "         '20200204_120214_singleparam_wbfcheck_wbf_0.1_inp_1',\n",
    "         '20200128_142303_singleparam_frzrtvarsm15_wbf_1_inp_100',\n",
    "         '20200116_130416_nudged_wbfmods_wbf_10_inp_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a single dataset of 15 mnths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = cases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5450055599212646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/xarray/backends/api.py:933: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "ds0 = xr.open_mfdataset('%s/%s/%s.nc' % (case_dir, case, case))#, combine='by_coords', chunks={'lat':10})\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4626476764678955\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "ds1 = xr.open_mfdataset('%s/%s/%s.nc' % (case_dir, case, case), combine='by_coords') #, chunks={'lat':10})\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1622588634490967\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "ds2 = xr.open_mfdataset('%s/%s/%s.nc' % (case_dir, case, case), combine='by_coords', chunks={'lat':10, 'time':1})\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will probably kill the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "ds3 = xr.open_mfdataset('%s/%s/%s.nc' % (case_dir, case, case), combine='by_coords', chunks={'lat':10, 'time':1, 'lev':1})\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking takes longer\n",
    "And causes the kernel to crash sometimes too. Weird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask does not change the time to load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ds0,ds1,ds2] # excluding d3 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just creating a new variable doesn't take long. Same time for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (time: 15, lat: 96, lon: 144)>\n",
      "dask.array<truediv, shape=(15, 96, 144), dtype=float32, chunksize=(15, 96, 144), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2000-02-01 2000-03-01 ... 2001-04-01\n",
      "  * lat      (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0\n",
      "  * lon      (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "0.006802797317504883\n",
      "<xarray.DataArray (time: 15, lat: 96, lon: 144)>\n",
      "dask.array<truediv, shape=(15, 96, 144), dtype=float32, chunksize=(15, 96, 144), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2000-02-01 2000-03-01 ... 2001-04-01\n",
      "  * lat      (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0\n",
      "  * lon      (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "0.005956888198852539\n",
      "<xarray.DataArray (time: 15, lat: 96, lon: 144)>\n",
      "dask.array<truediv, shape=(15, 96, 144), dtype=float32, chunksize=(1, 10, 144), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2000-02-01 2000-03-01 ... 2001-04-01\n",
      "  * lat      (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0\n",
      "  * lon      (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "0.0058553218841552734\n"
     ]
    }
   ],
   "source": [
    "for _ds in datasets:\n",
    "    t0 = time.time()    \n",
    "    new_var = _ds['TS']/_ds['PS']\n",
    "    print(new_var)\n",
    "    tf = time.time()\n",
    "    print(tf-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing and printing the new 2D variable doesn't take very long either. Weird..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012898921966552734\n",
      "0.00898432731628418\n",
      "0.2608156204223633\n"
     ]
    }
   ],
   "source": [
    "for _ds in datasets:\n",
    "    t0 = time.time()    \n",
    "    new_var = _ds['TS']/_ds['PS']\n",
    "    _blank = new_var.values\n",
    "    tf = time.time()\n",
    "    print(tf-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with a higher dimension variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002754688262939453\n",
      "0.0025043487548828125\n",
      "0.002923250198364258\n"
     ]
    }
   ],
   "source": [
    "for _ds in datasets:\n",
    "    t0 = time.time()    \n",
    "    new_var = _ds['AREI']/_ds['FREQI']\n",
    "    tf = time.time()\n",
    "    print(tf-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0692603588104248\n",
      "0.07379651069641113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n",
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n"
     ]
    }
   ],
   "source": [
    "for _ds in datasets[:-1]:\n",
    "    t0 = time.time()    \n",
    "    new_var = _ds['AREI']/_ds['FREQI']\n",
    "    _blank = new_var.values\n",
    "    tf = time.time()\n",
    "    print(tf-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 32, 96, 144)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_blank.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking seems to be worse all around\n",
    "Now just about any computation is killing the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first time, it takes much longer. So running several times hides the actual computational weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subselecting data does save significant time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n",
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n",
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06509041786193848\n",
      "0.01690053939819336\n",
      "0.00967717170715332\n",
      "0.07059884071350098\n",
      "0.018398761749267578\n",
      "0.010392904281616211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n",
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n",
      "/opt/conda/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*args2)\n"
     ]
    }
   ],
   "source": [
    "for _ds in datasets[:-1]:\n",
    "    somelats = _ds.sel(lat=slice(70,90))\n",
    "    sometimes = _ds.isel(time=0)\n",
    "    t0 = time.time()\n",
    "    new_var = _ds['AREI']/_ds['FREQI']\n",
    "    _blank = new_var.values\n",
    "    tf = time.time()\n",
    "    print(tf-t0)    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    new_var = somelats['AREI']/somelats['FREQI']\n",
    "    _blank = new_var.values\n",
    "    tf = time.time()\n",
    "    print(tf-t0)\n",
    "\n",
    "    t0 = time.time()\n",
    "    new_var = sometimes['AREI']/sometimes['FREQI']\n",
    "    _blank = new_var.values\n",
    "    tf = time.time()\n",
    "    print(tf-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
